{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y6/pdkrkx0s06d2k0_qhmmrpf8c0000gn/T/ipykernel_58573/2503121050.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_XLA_FLAGS'] = ''\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "import numpy as np\n",
    "from numpy import loadtxt, savetxt, asarray\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l1, l2, L1L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the reduced dataset, which has 15 features per each entry, and split it into three different sets for training, testing and validation. The features that we will consider are the rise times, fall times and the widths of the pulses for 5 different sensor regions. [PArise,PBrise,PCrise,PDrise,PFrise,PAfall,PBfall,PCfall,PDfall,PFfall,PAwidth,PBwidth,PCwidth,PDwidth,PFwidth]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/reduced_dataset/reduced_data.csv'\n",
    "org_train_df = pd.read_csv(filename)\n",
    "org_feature_df = org_train_df.drop(['Row', 'y','PBstart','PCstart','PDstart','PFstart'], axis=1)  # remove ID and postions/targets \n",
    "all_features = org_feature_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we normalize the features to have range between [0,1] using the `Standard Scalar` from `scikit` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(org_feature_df)\n",
    "\n",
    "feature_df_np = org_feature_df.to_numpy()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feature_df_np)\n",
    "scaled_feature_np = scaler.transform(feature_df_np)\n",
    "feature_df = pd.DataFrame(data=scaled_feature_np, columns=all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_hos_rmse(e):\n",
    "    return e[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7151, 15)\n",
      "(4508, 16) (1128, 16) (1515, 16)\n"
     ]
    }
   ],
   "source": [
    "filename = '../data/reduced_dataset/reduced_data.csv'\n",
    "\n",
    "dataset = loadtxt(filename,delimiter=',',skiprows=1)\n",
    "\n",
    "org_train_df = pd.read_csv(filename)\n",
    "org_feature_df = org_train_df.drop(['Row', 'y','PBstart','PCstart','PDstart','PFstart'], axis=1)  # remove ID and postions/targets \n",
    "#org_feature_df = org_train_df.drop(['Row', 'y','PAamp','PBamp','PCamp','PDamp','PFamp'], 1)\n",
    "all_features = org_feature_df.columns\n",
    "\n",
    "\n",
    "\n",
    "x = np.array(org_feature_df)\n",
    "\n",
    "y = dataset[:,len(dataset[0])-1]\n",
    "\n",
    "#---normalize features to be the range of [0,1]\n",
    "feature_df_np = org_feature_df.to_numpy()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feature_df_np)\n",
    "scaled_feature_np = scaler.transform(feature_df_np)\n",
    "feature_df = pd.DataFrame(data=scaled_feature_np, columns=all_features)\n",
    "\n",
    "x = np.array(feature_df)\n",
    "\n",
    "print(np.shape(x))\n",
    "\n",
    "y = dataset[:,len(dataset[0])-1]\n",
    "\n",
    "data = []\n",
    "test = []\n",
    "\n",
    "y_norm = -41.9\n",
    "\n",
    "for i in range(len(x)):\n",
    "    temp = []\n",
    "    for j in range(len(x[i])):\n",
    "        temp.append(x[i][j])\n",
    "    temp.append(float(y[i]))\n",
    "    temp_y = int(np.round(abs(y[i]),0))\n",
    "    if temp_y==13 or temp_y==30 or temp_y==42:\n",
    "        test.append(temp)\n",
    "    else:\n",
    "        data.append(temp)\n",
    "\n",
    "data = np.array(data)\n",
    "\n",
    "training, validation = train_test_split(data, test_size = 0.2, random_state = 42,stratify=data[:,-1])\n",
    "\n",
    "testing = np.array(test)\n",
    "\n",
    "training[:, [5, 1]] = training[:, [1, 5]]\n",
    "training[:, [10, 2]] = training[:, [2, 10]]\n",
    "training[:, [5, 3]] = training[:, [3, 5]]\n",
    "training[:, [6, 4]] = training[:, [4, 6]]\n",
    "training[:, [11, 5]] = training[:, [5, 11]]\n",
    "training[:, [10, 6]] = training[:, [6, 10]]\n",
    "training[:, [12, 8]] = training[:, [8, 12]]\n",
    "training[:, [11, 9]] = training[:, [9, 11]]\n",
    "training[:, [12, 10]] = training[:, [10, 12]]\n",
    "training[:, [13, 11]] = training[:, [11, 13]]\n",
    "\n",
    "validation[:, [5, 1]] = validation[:, [1, 5]]\n",
    "validation[:, [10, 2]] = validation[:, [2, 10]]\n",
    "validation[:, [5, 3]] = validation[:, [3, 5]]\n",
    "validation[:, [6, 4]] = validation[:, [4, 6]]\n",
    "validation[:, [11, 5]] = validation[:, [5, 11]]\n",
    "validation[:, [10, 6]] = validation[:, [6, 10]]\n",
    "validation[:, [12, 8]] = validation[:, [8, 12]]\n",
    "validation[:, [11, 9]] = validation[:, [9, 11]]\n",
    "validation[:, [12, 10]] = validation[:, [10, 12]]\n",
    "validation[:, [13, 11]] = validation[:, [11, 13]]\n",
    "\n",
    "testing[:, [5, 1]] = testing[:, [1, 5]]\n",
    "testing[:, [10, 2]] = testing[:, [2, 10]]\n",
    "testing[:, [5, 3]] = testing[:, [3, 5]]\n",
    "testing[:, [6, 4]] = testing[:, [4, 6]]\n",
    "testing[:, [11, 5]] = testing[:, [5, 11]]\n",
    "testing[:, [10, 6]] = testing[:, [6, 10]]\n",
    "testing[:, [12, 8]] = testing[:, [8, 12]]\n",
    "testing[:, [11, 9]] = testing[:, [9, 11]]\n",
    "testing[:, [12, 10]] = testing[:, [10, 12]]\n",
    "testing[:, [13, 11]] = testing[:, [11, 13]]\n",
    "\n",
    "\n",
    "np.random.shuffle(testing)\n",
    "np.random.shuffle(training)\n",
    "np.random.shuffle(validation)\n",
    "\n",
    "print(np.shape(training),np.shape(validation),np.shape(testing))\n",
    "\n",
    "savetxt('../data/processed_csv/conv_training_reduced.csv', asarray(training),fmt='%s', delimiter=',')\n",
    "savetxt('../data/processed_csv/conv_validation_reduced.csv', asarray(validation),fmt='%s', delimiter=',')\n",
    "savetxt('../data/processed_csv/conv_testing_reduced.csv', asarray(testing),fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = loadtxt('../data/processed_csv/conv_training_reduced.csv',delimiter=',')\n",
    "val_dataset = loadtxt('../data/processed_csv/conv_validation_reduced.csv',delimiter=',')\n",
    "test_dataset = loadtxt('../data/processed_csv/conv_testing_reduced.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_dataset[:,0:len(train_dataset[0])-1]\n",
    "y = train_dataset[:,len(train_dataset[0])-1:]\n",
    "\n",
    "X_val = val_dataset[:,0:len(val_dataset[0])-1]\n",
    "y_val = val_dataset[:,len(val_dataset[0])-1:]\n",
    "\n",
    "X_test = test_dataset[:,0:len(test_dataset[0])-1]\n",
    "y_test = test_dataset[:,len(test_dataset[0])-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = 0.1\n",
    "L2 = 0.1\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 50\n",
    "INPUT_DIM = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_3 (Conv1D)           (None, 3, 15)             90        \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 3, 15)             0         \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 3, 100)            35100     \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 3, 100)            0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 3, 50)             5050      \n",
      "                                                                 \n",
      " leaky_re_lu_12 (LeakyReLU)  (None, 3, 50)             0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3, 25)             1275      \n",
      "                                                                 \n",
      " leaky_re_lu_13 (LeakyReLU)  (None, 3, 25)             0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 3, 10)             260       \n",
      "                                                                 \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, 3, 10)             0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3, 1)              11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41786 (163.23 KB)\n",
      "Trainable params: 41786 (163.23 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "min_rmse = 1000\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "\n",
    "    #CNN\n",
    "    #'''\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv1D(filters=INPUT_DIM, kernel_size=5, strides=5,input_shape=(INPUT_DIM,1),kernel_regularizer=l1(L1)))\n",
    "    #model.add(tf.keras.layers.Dropout(dr))\n",
    "    #model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.GRU(100, return_sequences='false', kernel_regularizer=l1(L1)))\n",
    "    #model.add(tf.keras.layers.Dropout(dr))\n",
    "    #model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dense(units=50, kernel_regularizer=l1(L1)))\n",
    "    #model.add(tf.keras.layers.Dropout(dr))\n",
    "    #model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dense(units=25, kernel_regularizer=l1(L1)))\n",
    "    #model.add(tf.keras.layers.Dropout(dr))\n",
    "    #model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dense(units=10, kernel_regularizer=l1(L1)))\n",
    "    #model.add(tf.keras.layers.Dropout(dr))\n",
    "    #model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "    #'''\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(), metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=0,patience=1)\n",
    "    history = model.fit(X,y,epochs=i,batch_size=BATCH_SIZE,verbose=0,validation_data=(X_val,y_val),callbacks=[callback])\n",
    "\n",
    "    if i==0:\n",
    "        print(model.summary())\n",
    "        print('')\n",
    "    #model.save('test_DNN.h5')\n",
    "\n",
    "    train_mse, train_rmse = model.evaluate(X, y, batch_size=50,verbose=0)\n",
    "    val_mse, val_rmse = model.evaluate(X_val, y_val, batch_size=50,verbose=0)\n",
    "    test_mse, test_rmse = model.evaluate(X_test, y_test, batch_size=50,verbose=0)\n",
    "\n",
    "    print(train_rmse,val_rmse,test_rmse)\n",
    "    results.append((train_rmse,val_rmse,test_rmse))\n",
    "\n",
    "    if test_rmse<min_rmse:\n",
    "        model.save('CNN_FullNPA_L1RegES_noBN_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort(reverse=False,key=return_hos_rmse)\n",
    "np.savetxt('CNN_FullNPA_L1RegES_noBN_results.csv', np.asarray(results),fmt='%s', delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
